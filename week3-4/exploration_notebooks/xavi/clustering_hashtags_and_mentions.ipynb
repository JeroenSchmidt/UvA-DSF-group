{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"../../data/cp_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "In this notebook, we will determine for each tweet if it belongs to any of these classes:\n",
    "* Talks about Trump\n",
    "* Talks about Hillary\n",
    "* Talks about both\n",
    "* We couldn't define any of the other groups\n",
    "\n",
    "This it will be evaluated by checking keywords (hashtags and mentions) on the tweets, these keywords will be our features. Once each tweet is labeled, we can evaluate sentiment analysis or topic analysis, or even compare the data with demographic data, such as population or states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the features\n",
    "We will define a function to extract the features from each tweet. Our features will be hashtags and mentions, we will not distinguish one with another, we will treat them the same way. Therefore, we will remove *#* and *@* from the hashtags and mentions, and from now on, we will call them keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def features(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Get hashtags and mentions with regex\n",
    "    # I guess we can improve this by taking hashtags and mentions from the API\n",
    "    tags = re.findall('(^|\\s)([@#][\\w-]+)', tweet)\n",
    "    tags = [item[1] for item in tags]\n",
    "    \n",
    "    #Remove mentions and hashtags symbols\n",
    "    tags = list(map(lambda item: item\n",
    "                     .replace('@', '')\n",
    "                     .replace('#',''), tags))\n",
    "    \n",
    "    # Remove repetitive elements\n",
    "    tags = list(set(tags))\n",
    "    \n",
    "    # If the list is empty, return a NaN\n",
    "    return tags if tags else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function for each tweet and afterwards, remove those tweets that do not have any keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = tweets.text.apply(features)\n",
    "kw.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               [theblaze, realdonaldtrump]\n",
      "1         [barackobama, fbi, trumppence, lorettalynch, n...\n",
      "10                           [realdonaldtrump, mike4193496]\n",
      "100       [joenbc, morningjoe, williegeist, mike_pence, ...\n",
      "1000                                      [realdonaldtrump]\n",
      "10000     [realdonaldtrump, barackobama, grammybijou3, i...\n",
      "100000    [barackobama, mmqc15, walshfreedom, hillarycli...\n",
      "100001      [tnuts, trumpers, dt, foxnews, realdonaldtrump]\n",
      "100002      [realdonaldtrump, hillaryclinton, melaniatrump]\n",
      "100003        [supportourtroopsandvets, bigduhie1955, maga]\n",
      "Name: text, dtype: object\n",
      "\n",
      "We reduce the dataset from 657307 to 597024\n"
     ]
    }
   ],
   "source": [
    "print(kw.head(10))\n",
    "print(\"\\nWe reduce the dataset from\",len(tweets), \"to\", len(kw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that given *n* will give us a list of the *n* most common keywords. This list will be used to build a binary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realdonaldtrump', 'hillaryclinton', 'trump', 'foxnews', 'cnn', 'nevertrump', 'maga', 'imwithher', 'neverhillary', 'hillary', 'trumppence16', 'donaldtrump', 'crookedhillary', 'seanhannity', 'kellyannepolls', 'msnbc', 'potus', 'gop', 'trumptrain', 'epn', 'nytimes', 'politico', 'mike_pence', 'reince', 'morning_joe', 'dumptrump', 'makeamericagreatagain', 'danscavino', 'erictrump', 'trump2016', 'donaldjtrumpjr', 'abc', 'basketofdeplorables', 'cnnpolitics', 'barackobama', 'greta', 'teamtrump', 'billclinton', 'timkaine', 'loudobbs', 'morningmika', 'tcot', 'rogerjstonejr', 'ingrahamangle', 'maddow', 'katrinapierson', 'washingtonpost', 'speakerryan', 'hfa', 'anncoulter']\n"
     ]
    }
   ],
   "source": [
    "def getNKeywords(kw, n):\n",
    "    output = []\n",
    "    dic = Counter()\n",
    "    for key, keywords in kw.iteritems():\n",
    "        for k in keywords: dic[k] += 1\n",
    "            \n",
    "    output = sorted(dic, key=dic.__getitem__, reverse=True)\n",
    "    return output[:n]\n",
    "\n",
    "# Getting the N most common keywords\n",
    "all_keywords = getNKeywords(kw, 50)\n",
    "print(all_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a binary matrix\n",
    "In order to fit the clustering method, we will need a matrix $N x M$ and right now what we have so far is a list of list which the inside list has different lenght. That is the reason we will build a binary matrix where each row is a tweet and each column is a keyword from the list extracted before. The cell will indicated if that tweet has that keyword (True) or not (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = kw.apply(lambda items : np.array([keyword in items for keyword in all_keywords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tweet does not contain any keyword as we reduced the total keywords that we have, then we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We reduce the dataset from 597024 to 573745\n"
     ]
    }
   ],
   "source": [
    "reduced_matrix = matrix.apply(lambda x : x if True in x else np.nan)\n",
    "reduced_matrix.dropna(inplace=True)\n",
    "print(\"\\nWe reduce the dataset from\",matrix.shape[0], \"to\", reduced_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "Out main goal is to classify a tweet in these 4 groups:\n",
    "* Talks about Trump\n",
    "* Talks about Hillary\n",
    "* Talks about both\n",
    "* It's not relevant\n",
    "    \n",
    "For that, we will use a clustering method, K-means. K-means is a method for cluster analysis where you can give the number of cluster you want and it defines centroids which determine the 'middle' of a group (or class). This method normally even the clusters size which is not our case as we want 4 groups were the big one it will be the one that talks about Trump. Therefore, we decided to apply K-means but with *n_clusters* bigger than 4, let's say 15 and then we can classify manually if the cluster talks about Trump, Hillary or both.\n",
    "\n",
    "We should not forget we are trying to divide keywords (hashtags and mentions mainly) into groups, so each group will have a 'top-5' keywords that are used from that class. Once we know which are the most common keywords for each group, we can decide who are they talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=15, random_state=0, algorithm='full').fit(reduced_matrix.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the centroids tell us\n",
    "The centroids from the clustering will tell us what is the proability for each hashtag that appears in the tweets from that cluster. In orther words if the hashtag has a value of 1 that means that that hashtag appears in all the tweets from that group. The same if we have 0.45, there is a 45% chance that the hashtag appear in a selected random tweet from that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00000000e+00,  8.06466005e-13,  1.06339452e-02, -6.78207490e-14,\n",
       "        2.04489203e-14,  1.95433947e-13,  7.43918815e-14, -7.33857419e-14,\n",
       "        1.62479444e-03, -5.52335955e-15,  7.07356031e-03,  2.62799682e-03,\n",
       "        2.96633174e-03,  5.77524140e-14,  2.09374238e-02,  5.97987301e-03,\n",
       "        6.20018412e-03,  8.37182221e-03,  2.94666111e-03,  1.53116222e-02,\n",
       "        9.94547300e-03,  1.00398920e-02,  1.17945127e-02,  6.95160237e-03,\n",
       "        8.98554602e-03,  1.75068651e-03,  6.65260872e-03,  1.05001849e-02,\n",
       "        8.94227062e-03,  1.91985397e-03,  8.80064205e-03,  1.98280000e-03,\n",
       "        1.79396190e-03,  3.99313888e-03,  3.27712777e-03,  4.31967142e-03,\n",
       "        5.47630475e-03,  1.00320238e-03,  1.07008254e-03,  6.35754920e-03,\n",
       "        7.49451189e-03,  4.95699999e-04,  1.12122619e-03,  4.09149206e-03,\n",
       "        2.77749365e-03,  6.09002856e-03,  3.09615793e-03,  4.09936031e-03,\n",
       "        5.78316666e-04,  4.53998253e-03])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the first centroid, there is a probability of 1 that 'realdonaldtrump' appears on the tweets from that cluster and there is low chance for 'hillaryclinton', 'foxnews', 'cnn', 'nevertrump', 'maga', 'imwithher' appearing. The order preserved in the centroids is the same as the variable *all_keywords*, that is how we know which keyword belongs each number.\n",
    "\n",
    "We have to define a threshold to determine which keywords are significant for each class, this is just to visualise and clarify what is the cluster talking about, but the K-means algorithm is already applied. 'Undefined group' means the class that doesn't have any significant keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'keywords': ['realdonaldtrump'], 'tweets': 254186},\n",
       " 1: {'keywords': ['trump'], 'tweets': 48855},\n",
       " 2: {'keywords': ['hillaryclinton'], 'tweets': 111544},\n",
       " 3: {'keywords': ['nevertrump'], 'tweets': 16926},\n",
       " 4: {'keywords': ['donaldtrump'], 'tweets': 10733},\n",
       " 5: {'keywords': ['hillary'], 'tweets': 9684},\n",
       " 6: {'keywords': ['realdonaldtrump', 'hillaryclinton'], 'tweets': 25779},\n",
       " 7: {'keywords': ['foxnews'], 'tweets': 13494},\n",
       " 8: {'keywords': ['maga'], 'tweets': 16798},\n",
       " 9: {'keywords': ['seanhannity'], 'tweets': 4890},\n",
       " 10: {'keywords': ['imwithher'], 'tweets': 14116},\n",
       " 11: {'keywords': ['cnn'], 'tweets': 12846},\n",
       " 12: {'keywords': ['neverhillary'], 'tweets': 9367},\n",
       " 13: {'keywords': 'Undefined group', 'tweets': 18152},\n",
       " 14: {'keywords': ['hillaryclinton', 'foxnews'], 'tweets': 6375}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.95 # Threshold probability. It will give us the hashtags that its chance is bigger than p.\n",
    "c = 0\n",
    "clusters = {}\n",
    "counters = Counter(cluster.labels_)\n",
    "for center in cluster.cluster_centers_:\n",
    "    s = pd.Series(center)\n",
    "    idxs = s[s>=p].index\n",
    "    clusters[c] = {}\n",
    "    clusters[c]['keywords'] = [all_keywords[i] for i in idxs] if idxs.tolist() else 'Undefined group'\n",
    "    clusters[c]['tweets'] = counters[c] \n",
    "    c+=1\n",
    "        \n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling some tweets with its class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that given *n* tweets, we'll see which class belongs that tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@DanScavino @realDonaldTrump 199\n",
      "['realdonaldtrump'] \n",
      "\n",
      "@realDonaldTrump liar liar pants on fire!\n",
      "['realdonaldtrump'] \n",
      "\n",
      "@ananavarro @realDonaldTrump https://t.co/rIHjwzkTo7\n",
      "['realdonaldtrump'] \n",
      "\n",
      "Well THIS is worth sharing, too!Thanks, @PeterHase2014  #NeverTrump https://t.co/0DwCeMsVfp\n",
      "['nevertrump'] \n",
      "\n",
      "So Maddow made Conway look like a fool defending #Trump and unconstitutional measures! bwahaha!\n",
      "['trump'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "for i in range(n):\n",
    "    i = randint(0, len(cluster.labels_))\n",
    "    idx = reduced_matrix.index.tolist()[i]\n",
    "    print(tweets.at[idx, 'text'])\n",
    "    print(clusters[cluster.labels_[i]]['keywords'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the classes from K-means to our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the classes obtained from K-means, we can decide manually what is the tweet talking about (Trump, Hillary, both or irrelevant), knowing the most common words in that class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
